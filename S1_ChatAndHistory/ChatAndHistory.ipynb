{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7d6e9013",
   "metadata": {},
   "source": [
    "### Setup Langchain + LLM\n",
    "1. Install Langchain: \n",
    "- pip intall langchain\n",
    "2. Install integration packages.\n",
    "- pip install -U langchain-cohere\n",
    "- pip install -U langchain-groq\n",
    "- pip install -U langchain-mistralai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bfff63ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='Hi there! I can help you with that. \\n\\nPlease provide me with your location, and I will give you the current weather conditions and the forecast for the rest of the day. \\n\\n- Have a great day!' additional_kwargs={'documents': None, 'citations': None, 'search_results': None, 'search_queries': None, 'is_search_required': None, 'generation_id': '1fcfcc9d-7440-4148-a424-eddeab77e31d', 'token_count': {'input_tokens': 237.0, 'output_tokens': 46.0}} response_metadata={'documents': None, 'citations': None, 'search_results': None, 'search_queries': None, 'is_search_required': None, 'generation_id': '1fcfcc9d-7440-4148-a424-eddeab77e31d', 'token_count': {'input_tokens': 237.0, 'output_tokens': 46.0}} id='run-5517e0f5-9206-4440-96f6-08f32edd2daf-0' usage_metadata={'input_tokens': 237, 'output_tokens': 46, 'total_tokens': 283}\n",
      "content=\"I'd be happy to help you with that! As of now, the current weather conditions are quite pleasant. There's a gentle breeze blowing, and the temperature is a comfortable 72°F (22°C). The skies are mostly clear, with just a few wispy clouds drifting lazily across the horizon. The humidity is relatively low, making it a great day to get outside and enjoy the fresh air. Have a great day!\" additional_kwargs={} response_metadata={'token_usage': {'completion_tokens': 88, 'prompt_tokens': 51, 'total_tokens': 139, 'completion_time': 0.073333333, 'prompt_time': 0.002099476, 'queue_time': 0.012266593000000001, 'total_time': 0.075432809}, 'model_name': 'llama3-8b-8192', 'system_fingerprint': 'fp_179b0f92c9', 'finish_reason': 'stop', 'logprobs': None} id='run-05032646-a37e-4b03-8329-8b55369e8114-0' usage_metadata={'input_tokens': 51, 'output_tokens': 88, 'total_tokens': 139}\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import configparser\n",
    "\n",
    "from langchain_groq import ChatGroq\n",
    "from langchain_cohere import ChatCohere\n",
    "\n",
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "\n",
    "config = configparser.ConfigParser()\n",
    "config.read('/workspaces/langchainCDAC9nov/config.ini')\n",
    "groq = config['groq']\n",
    "cohere = config['cohere']\n",
    "\n",
    "os.environ['GROQ_API_KEY'] = groq.get('GROQ_API_KEY')\n",
    "os.environ['COHERE_API_KEY'] = cohere.get('COHERE_API_KEY')\n",
    "\n",
    "messages = [\n",
    "    SystemMessage(content='You are a weather service. You will respond to weather queries to the best of you ability. You will always end with - Have a great day'),\n",
    "    HumanMessage(content='Hey whats the weather like today?')\n",
    "]\n",
    "\n",
    "\n",
    "\n",
    "## code for cohere.\n",
    "model = ChatCohere(model=\"command-r-plus\")\n",
    "print(model.invoke(messages))\n",
    "\n",
    "## Code for Groq\n",
    "model = ChatGroq(model=\"llama3-8b-8192\")\n",
    "print(model.invoke(messages))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2f6ad73b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='Delhi is the capital of India.' additional_kwargs={} response_metadata={'token_usage': {'completion_tokens': 9, 'prompt_tokens': 46, 'total_tokens': 55, 'completion_time': 0.0075, 'prompt_time': 0.008492568, 'queue_time': 0.006163749999999999, 'total_time': 0.015992568}, 'model_name': 'llama3-8b-8192', 'system_fingerprint': 'fp_6a6771ae9c', 'finish_reason': 'stop', 'logprobs': None} id='run-c41994e3-1352-433b-b151-159898307055-0' usage_metadata={'input_tokens': 46, 'output_tokens': 9, 'total_tokens': 55}\n"
     ]
    }
   ],
   "source": [
    "messages = [\n",
    "    SystemMessage(content='You are a Capital_City service. You will tell the Capital_City of the State. Have a great day!!'),\n",
    "    HumanMessage(content='Delhi is Capital of which Country')\n",
    "]\n",
    "# model = ChatCohere(model=\"command-r-plus\")\n",
    "# print(model.invoke(messages))\n",
    "\n",
    "# Code for Groq\n",
    "model = ChatGroq(model=\"llama3-8b-8192\")\n",
    "print(model.invoke(messages))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e76fa0d3-c23a-455f-998f-bb68018d0db0",
   "metadata": {},
   "source": [
    "## Create the prompt\n",
    "1. Imports Human and System message classes. System represents our instructions to GPT and Human represents the question or prompt that the user provides.\n",
    "2. LangChain responses are instances of class `BaseMessage` It contains the actual response from GPT and some other metadata.\n",
    "3. Since we are interested only in the string reponse that GPT gave we chain (pipe) the reponse to a parser\n",
    "4. For our purpose we will use `StrOutputParser` class\n",
    "5. Next we create a `chain` using the components `model` and `parser`\n",
    "6. Finally we call the `invoke` method on the chain and pass our `messages` list to it.\n",
    "7. In the output cell we get the response from `GPT-35-turbo`\n",
    "\n",
    "*A chain is an wrapper around multiple individual components that are executed in a defined order. Components in LangChain implement `Runnable` interface. This interface have a method `invoke` that transforms single input to an output.*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "36f9fb1d-d604-41ab-8e62-5ea4e6a9213b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "client=<cohere.client.Client object at 0x7ad2e8a910d0> async_client=<cohere.client.AsyncClient object at 0x7ad2e8a918e0> model='command-r' cohere_api_key=SecretStr('**********')\n",
      "['/usr/local/python/3.12.1/lib/python312.zip', '/usr/local/python/3.12.1/lib/python3.12', '/usr/local/python/3.12.1/lib/python3.12/lib-dynload', '', '/home/codespace/.local/lib/python3.12/site-packages', '/usr/local/python/3.12.1/lib/python3.12/site-packages']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"This classic puzzle is a great example of lateral thinking! The man should choose the third room, full of tigers that haven't eaten for six months. Why? Because tigers that haven't eaten for six months are likely to be dead or very weak, making them no threat to the man's life. The firing squad and blazing fire are much more likely to result in his death, so the third room is the safest choice.\""
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#The classes used for setting up the prompt\n",
    "import puzzles\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate #import the Class for creating a prompt\n",
    "\n",
    "parser = StrOutputParser()\n",
    "\n",
    "puzzle = puzzles.puzzles('hungryLions') # Based on user input pick a puzzle.\n",
    "\n",
    "# templatized system prompt\n",
    "system_template = \"solve the following puzzle. Please provide a {responseType} response.\" \n",
    "\n",
    "# Create prompt template instance.\n",
    "prompt_template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system_template),\n",
    "        (\"user\", puzzle)\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "# prompt Template also implements runnable and can be easily chained.\n",
    "model = ChatGroq(model=\"llama3-8b-8192\")\n",
    "chain = prompt_template | model | parser\n",
    "\n",
    "chain.invoke({\"responseType\":\"100 words\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5067ffb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "74ab938d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Pune': [{'name': 'Agakhan Palace', 'ranking': 4.8, 'reviews': 1234},\n",
       "  {'name': 'Shani Shingnapur', 'ranking': 4.6, 'reviews': 987},\n",
       "  {'name': 'Osho Teerth Park', 'ranking': 4.5, 'reviews': 765},\n",
       "  {'name': 'Pune Okayama Friendship Garden', 'ranking': 4.4, 'reviews': 543},\n",
       "  {'name': 'Bhaja Caves', 'ranking': 4.3, 'reviews': 421}]}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "parser = JsonOutputParser()\n",
    "\n",
    "# puzzle = puzzles.puzzles('hungryLions') # Based on user input pick a puzzle.\n",
    "query=input('Enter the city for which you want the tourist places:  ')\n",
    "\n",
    "# templatized system prompt\n",
    "system_template = \"return a output in json form as the tourist place in city and its ranking according to reviews as per the input city. Please provide a {size} related response.\" \n",
    "\n",
    "# Create prompt template instance.\n",
    "prompt_template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system_template),\n",
    "        (\"user\", query)\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "# prompt Template also implements runnable and can be easily chained.\n",
    "model = ChatGroq(model=\"llama3-8b-8192\")\n",
    "chain = prompt_template | model | parser\n",
    "\n",
    "chain.invoke({\"responseType\":\"brief\",\"size\":input(\"Enter the size you expect\")})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9498ce4c",
   "metadata": {},
   "source": [
    "### Chatbot \n",
    "1. We begin with creating a basic chatbot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7a61142d-54d4-46b8-a50a-3d1b473e82a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hi Bob! It's nice to meet you. Is there something I can help you with or would you like to chat?\n"
     ]
    }
   ],
   "source": [
    "model = ChatGroq(model=\"llama3-8b-8192\")\n",
    "parser = StrOutputParser()\n",
    "chain = model | parser\n",
    "\n",
    "response = chain.invoke([HumanMessage(content=\"hi I am Bob\")])\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "582ff0e7",
   "metadata": {},
   "source": [
    "#### Lets dig into what is happening here.\n",
    "1. Click here to check the UML diagram: \n",
    "2. https://medium.com/azure-monitor-from-a-programmers-perspective/langchain-ii-basic-chatbot-unpacked-a60510b9ac6b#56cf\n",
    "\n",
    "\n",
    "#### Runnable\n",
    "1. Its an extremely prominent class and used extensively in creating chains.\n",
    "2. Chains combine components together in a pipeline\n",
    "3. Many components like all models, parsers, prompts and anything that can logically go into a chain derives from it.\n",
    "4. `ChatGroq` is provided partner by extends `BaseChatModel` from langchain_core\n",
    "5. https://github.com/langchain-ai/langchain/blob/master/libs/partners/groq/langchain_groq/chat_models.py\n",
    "6. This is the base class for all model classes offered by any partner.\n",
    "7. `BaseClass` extends `RunnableSerializable` that supports serialization into JSON\n",
    "8. `RunnableSerializable` extends `Runnable` that means it can participate in chains.\n",
    "9. You can also use `RunnableSequence` to construct the chain.\n",
    "10. https://github.com/langchain-ai/langchain/blob/master/libs/core/langchain_core/runnables/base.py#L2659"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85ecfeb8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Hi Bob! It's nice to meet you. Is there something I can help you with or would you like to chat?\""
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.runnables import RunnableSequence\n",
    "chain = RunnableSequence(model, parser)\n",
    "chain.invoke([HumanMessage(content=\"hi i am bob\")])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe9fbe39",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Nice to meet you, Atharva! How's your day going so far?\""
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.runnables import RunnableSequence\n",
    "chain = RunnableSequence(model,parser)\n",
    "chain.invoke([HumanMessage(content=\"Hii I am Atharva\")])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a7c363b",
   "metadata": {},
   "source": [
    "1. Chain calls the first component and passes any arguments provided to it.\n",
    "2. In this case its an object of type `HumanMessage`\n",
    "3. This is how a chain looks: https://miro.medium.com/v2/resize:fit:750/format:webp/1*K1F-m4gImEUO0AELkpQuKg.jpeg\n",
    "4. Each model component by any partner provides an object of type `BaseMessage`. This is then passed to the next component.\n",
    "5. This is the signature of invoke of a model class\n",
    "\n",
    "`def` `invoke(str | List[dict | tuple | BaseMessage] | PromptValue):`\\\n",
    "    Suite\n",
    "  \n",
    "6. In our example `HumanMessage` is derived from `BaseMessage` which needs `content` for initialization.\n",
    "\n",
    "`param content: Union[str, List[Union[str,Dict]]]`\n",
    "\n",
    "7. Union, List, Dict are all defined in typing module\n",
    "8. Union means one of the input types is expected. We are passing a string.\n",
    "\n",
    "9. Our `parser` is of type `StrOutputParser` that extends `BaseOutputParser`\n",
    "10. Its invoke is:\n",
    "\n",
    "`def invoke(self, input: Union[str, BaseMessage], config: Optional[RunnableConfig] = None) -> T:`\n",
    "\n",
    "11.  This says input can be either string or `BaseMessage`. We are using `BaseMessage` the return type of `model`\n",
    "\n",
    "12. Some useful methods are:\n",
    "- parser.input_schema.schema() # get JSON schema of the input\n",
    "- parser.output_schema.schema() # gets JSON schema of the output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "807a9b92",
   "metadata": {},
   "source": [
    "### Adding history to chat\n",
    "1. At this stage if you pass another message to the model it will have no recollection of the earlier message.\n",
    "2. Lets add history. Chat history is managed by a set of classes offered by community.\n",
    "3. https://github.com/langchain-ai/langchain/blob/master/libs/core/langchain_core/chat_history.py\n",
    "4. `asyncio` is a Python library: https://docs.python.org/3/library/asyncio.html "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c94cf4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Messages retrieved from DB\n",
      "Hi Bob! Nice to meet you! Is there something I can help you with or would you like to chat?\n",
      "Messages retrieved from DB\n",
      "[HumanMessage(content='Hi! I am Bob', additional_kwargs={}, response_metadata={}), SystemMessage(content='Hi Bob! Nice to meet you! Is there something I can help you with or would you like to chat?', additional_kwargs={}, response_metadata={}), HumanMessage(content='Lets see if you know my name dude?', additional_kwargs={}, response_metadata={})]\n",
      "I knew it! Your name is Bob, right?\n"
     ]
    }
   ],
   "source": [
    "# import the chat history classes\n",
    "from langchain_core.chat_history import (\n",
    "    BaseChatMessageHistory,\n",
    "    InMemoryChatMessageHistory,\n",
    ")\n",
    "import asyncio # library for writing code that interacts with DB, network calls etc. \n",
    "\n",
    "#Create a store in memory\n",
    "store = InMemoryChatMessageHistory()\n",
    "\n",
    "\n",
    "# Lets define a function that gets messages from store\n",
    "async def getMessage():\n",
    "    await asyncio.sleep(2) # this will mimic a read from DB\n",
    "    print(\"Messages retrieved from DB\")\n",
    "    return await store.aget_messages()\n",
    "\n",
    "# Now lets first add the first message to the store\n",
    "store.add_message(HumanMessage('Hi! I am Bob'))\n",
    "\n",
    "messages = await(getMessage())\n",
    "\n",
    "\n",
    "response = model.invoke(messages) # asyncio has runners for coroutines, context managers etc. \n",
    "print(response.content) # note that our first message is safely in the store\n",
    "\n",
    "# lets add the message returned by the model to the store\n",
    "\n",
    "store.add_message(SystemMessage(response.content))\n",
    "\n",
    "\n",
    "store.add_message(HumanMessage('Lets see if you know my name dude?'))\n",
    "\n",
    "messages = await(getMessage())\n",
    "\n",
    "print(messages) # check all the message are in store.\n",
    "\n",
    "response = model.invoke(messages)\n",
    "\n",
    "print(response.content) # Notice that the reponse now takes into account earlier interactions also."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65c8feba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Messages retrieved\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namaste Atharva! It's great to meet you. Maharashtra is a beautiful state with a rich culture and history. What brings you here today? Do you have any interests or hobbies you'd like to share?\n",
      "Messages retrieved\n",
      "[HumanMessage(content='I am Atharva and I am from Maharashtra', additional_kwargs={}, response_metadata={}), SystemMessage(content=\"Namaste Atharva! It's great to meet you. Maharashtra is a beautiful state with a rich culture and history. What brings you here today? Do you have any interests or hobbies you'd like to share?\", additional_kwargs={}, response_metadata={}), HumanMessage(content='Lets see if you can tell capital city of my state?', additional_kwargs={}, response_metadata={})]\n",
      "That's a great question! I think the capital city of Maharashtra is... Mumbai! Am I correct?\n"
     ]
    }
   ],
   "source": [
    "# model2 = ChatGroq(model=\"llama3-8b-8192\")\n",
    "from langchain_core.chat_history import(BaseChatMessageHistory,InMemoryChatMessageHistory,)\n",
    "store=InMemoryChatMessageHistory()\n",
    "import asyncio\n",
    "async def getMessage():\n",
    "    await asyncio.sleep(2)\n",
    "    print(\"Messages retrieved\")\n",
    "    return await store.aget_messages()\n",
    "store.add_message(HumanMessage(\"I am Atharva and I am from Maharashtra\"))\n",
    "messages = await(getMessage())\n",
    "response=model.invoke(messages)\n",
    "print(response.content)\n",
    "store.add_message(SystemMessage(response.content))\n",
    "\n",
    "\n",
    "store.add_message(HumanMessage('Lets see if you can tell capital city of my state?'))\n",
    "\n",
    "messages = await(getMessage())\n",
    "print(messages)\n",
    "response = model.invoke(messages)\n",
    "\n",
    "print(response.content) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "448f1a76",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0e221d9",
   "metadata": {},
   "source": [
    "1. There are some issues here. Since Chat History is not a descendant of Runnable we cannot chain it.\n",
    "2. Therefore the code is sort of littered. \n",
    "3. Also we are required to write functions for storing and retrieving messages. This should be rather standard and done by the framework!\n",
    "4. What about sessions? This code is running of the server which supports multiple users. So there needs to be a mechanism to manage sessions.\n",
    "\n",
    "#### RunnableWithMessageHistory\n",
    "1. This is where LangChain offers this class.\n",
    "2. It takes the chain as the first argument and a pointer to the store get method as the second argument.\n",
    "3. This class then takes the ownership of executing the chain and any component that "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "949191e4",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'BaseChatMessageHistory' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 7\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain_core\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mrunnables\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mhistory\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m RunnableWithMessageHistory\n\u001b[1;32m      5\u001b[0m store \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m----> 7\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_session_history\u001b[39m(session_id: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[43mBaseChatMessageHistory\u001b[49m:\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m session_id \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m store:  \u001b[38;5;66;03m# If a new session then create a new memory store.\u001b[39;00m\n\u001b[1;32m      9\u001b[0m         store[session_id] \u001b[38;5;241m=\u001b[39m InMemoryChatMessageHistory()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'BaseChatMessageHistory' is not defined"
     ]
    }
   ],
   "source": [
    "# Lets create our own store. This store will be a dict with a key for each session\n",
    "# The value for each key will be InMemoryChatHistory object \n",
    "\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "store = {}\n",
    "\n",
    "def get_session_history(session_id: str) -> BaseChatMessageHistory:\n",
    "    if session_id not in store:  # If a new session then create a new memory store.\n",
    "        store[session_id] = InMemoryChatMessageHistory()\n",
    "    return store[session_id]\n",
    "config = {'configurable': {\"session_id\": \"abc2\"}}\n",
    "withHistory = RunnableWithMessageHistory(model, get_session_history)\n",
    "\n",
    "response = withHistory.invoke([HumanMessage(content=\"Hi! I am Bob\")], config=config)\n",
    "\n",
    "print(response.content) # all good so far\n",
    "\n",
    "# we dont need to explicitly store the response from the model in history\n",
    "\n",
    "response = withHistory.invoke(\n",
    "    [HumanMessage(content=\"Lets see if you know my name dude?\")], config=config\n",
    ")\n",
    "\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "a51e8530",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nice to meet you, Atharva! Pune is a great city, isn't it? What do you like most about living in Pune?\n",
      "I remember that your name is Atharva, and you're living in Pune.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "store = {}\n",
    "\n",
    "def get_session_history(session_id: str) -> BaseChatMessageHistory:\n",
    "    if session_id not in store:  # If a new session then create a new memory store.\n",
    "        store[session_id] = InMemoryChatMessageHistory()\n",
    "    return store[session_id]\n",
    "config = {'configurable': {\"session_id\": \"Sesssion-1\"}}\n",
    "newHistory = RunnableWithMessageHistory(model, get_session_history)\n",
    "\n",
    "response = newHistory.invoke([HumanMessage(content=\"Hello I am Atharva and I am living in Pune\")], config=config)\n",
    "\n",
    "print(response.content) # all good so far\n",
    "\n",
    "# we dont need to explicitly store the response from the model in history\n",
    "\n",
    "response = withHistory.invoke(\n",
    "    [HumanMessage(content=\"Do you remember my name?\")], config=config\n",
    ")\n",
    "\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb8e719e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I remember! You're Rajan, and you're living in Mumbai!\n",
      "I remember again! You said you belong in Mumbai!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "def get_session_history(session_id: str) -> BaseChatMessageHistory:\n",
    "    if session_id not in store:  # If a new session then create a new memory store.\n",
    "        store[session_id] = InMemoryChatMessageHistory()\n",
    "    return store[session_id]\n",
    "config = {'configurable': {\"session_id\": \"Session-2\"}}\n",
    "newHistory = RunnableWithMessageHistory(model, get_session_history)\n",
    "\n",
    "response = newHistory.invoke([HumanMessage(content=\"Hello I am Rajan and I am living in Mumbai\")], config=config)\n",
    "\n",
    "print(response.content) # all good so far\n",
    "\n",
    "# we dont need to explicitly store the response from the model in history\n",
    "\n",
    "response = withHistory.invoke(\n",
    "    [HumanMessage(content=\"Do you remember where do I belong ?\")], config=config\n",
    ")\n",
    "\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "9889e4a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['Sesssion-1', 'Session-2'])\n"
     ]
    }
   ],
   "source": [
    "print(store.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "d6eec3d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Sesssion-1': InMemoryChatMessageHistory(messages=[HumanMessage(content='Hello I am Atharva and I am living in Pune', additional_kwargs={}, response_metadata={}), AIMessage(content=\"Nice to meet you, Atharva! Pune is a great city, isn't it? What do you like most about living in Pune?\", additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 30, 'prompt_tokens': 22, 'total_tokens': 52, 'completion_time': 0.025, 'prompt_time': 0.000784625, 'queue_time': 0.012611942999999999, 'total_time': 0.025784625}, 'model_name': 'llama3-8b-8192', 'system_fingerprint': 'fp_179b0f92c9', 'finish_reason': 'stop', 'logprobs': None}, id='run-ac94c00e-5e52-43bd-84e8-7f71738aa45e-0', usage_metadata={'input_tokens': 22, 'output_tokens': 30, 'total_tokens': 52}), HumanMessage(content='Do you remember my name?', additional_kwargs={}, response_metadata={}), AIMessage(content=\"I remember that your name is Atharva, and you're living in Pune.\", additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 18, 'prompt_tokens': 67, 'total_tokens': 85, 'completion_time': 0.015, 'prompt_time': 0.012026484, 'queue_time': 0.001771403999999999, 'total_time': 0.027026484}, 'model_name': 'llama3-8b-8192', 'system_fingerprint': 'fp_6a6771ae9c', 'finish_reason': 'stop', 'logprobs': None}, id='run-d51cf3cd-fe3b-4670-bbc9-f97f55c2cda8-0', usage_metadata={'input_tokens': 67, 'output_tokens': 18, 'total_tokens': 85})]), 'Session-2': InMemoryChatMessageHistory(messages=[HumanMessage(content='Hello I am Rajan and I am living in Mumbai', additional_kwargs={}, response_metadata={}), AIMessage(content=\"Hello Rajan! It's nice to meet you! Mumbai is a bustling city, isn't it? What do you like most about living there?\", additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 31, 'prompt_tokens': 21, 'total_tokens': 52, 'completion_time': 0.025833333, 'prompt_time': 0.000810525, 'queue_time': 0.014039424, 'total_time': 0.026643858}, 'model_name': 'llama3-8b-8192', 'system_fingerprint': 'fp_179b0f92c9', 'finish_reason': 'stop', 'logprobs': None}, id='run-da7efdf0-40c3-48ac-a91e-04321f03d970-0', usage_metadata={'input_tokens': 21, 'output_tokens': 31, 'total_tokens': 52}), HumanMessage(content='Do you remember where do I belong ?', additional_kwargs={}, response_metadata={}), AIMessage(content=\"I think I do! You said you're living in Mumbai, right?\", additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 16, 'prompt_tokens': 69, 'total_tokens': 85, 'completion_time': 0.013333333, 'prompt_time': 0.008300311, 'queue_time': 0.006403477000000001, 'total_time': 0.021633644}, 'model_name': 'llama3-8b-8192', 'system_fingerprint': 'fp_a97cfe35ae', 'finish_reason': 'stop', 'logprobs': None}, id='run-dde9312d-3e21-4c0a-9657-236cecb9a1fa-0', usage_metadata={'input_tokens': 69, 'output_tokens': 16, 'total_tokens': 85})])}\n"
     ]
    }
   ],
   "source": [
    "print(store)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c080dd6",
   "metadata": {},
   "source": [
    "1. Here is a flowchart of this program.\n",
    "2. https://medium.com/azure-monitor-from-a-programmers-perspective/langchain-ii-basic-chatbot-unpacked-a60510b9ac6b#3c92\n",
    "3. Wrapper around another runnable - the chain\n",
    "4. https://techblogs.cloudlex.com/langchain-ii-basic-chatbot-unpacked-a60510b9ac6b#a0cb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f6d9303",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
